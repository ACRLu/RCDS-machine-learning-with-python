{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dca1e0",
   "metadata": {},
   "source": [
    "# Machine Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae8111",
   "metadata": {},
   "source": [
    "## 1.2 Data\n",
    "\n",
    "To get started, let's get hold of some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627559a",
   "metadata": {},
   "source": [
    "### [Toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)\n",
    "\n",
    "We have some small demonstration datasets immediately available in `sklearn.datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d23021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feed88d",
   "metadata": {},
   "source": [
    "The data is returned as a `Bunch` object. This is similar to a dictionary, but also allows values to be referenced as attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris[\"filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6722078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a4cbc",
   "metadata": {},
   "source": [
    "Notice that the `data` and the `target` are provided separately as numpy arrays. Each row is an observation (i.e. a data point) and each column is a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e532a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33685ae",
   "metadata": {},
   "source": [
    "Use the `DESCR` attribute to find out more about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b0262",
   "metadata": {},
   "source": [
    "### Splitting testing from training data\n",
    "\n",
    "Before we even look at the data, it is good practice to split off a test dataset that will remain unseen until we are ready for final evaluation of our models.\n",
    "\n",
    "sklearn has a convenient function `train_test_split` that will create a randomised 75%:25% split of training:testing data. The `X` values are the features and the `y` values are the target.\n",
    "\n",
    "Notice that we can \"seed\" the random number generator to create deterministic output - this can be helpful during code development as it means our results will not change between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea118c44",
   "metadata": {},
   "source": [
    "### Visualising data\n",
    "\n",
    "Before we start, it is good practice to take a look at the general form of the data to identify any inconsistencies or errors.\n",
    "\n",
    "We can use the `scatter_matrix` from pandas to create a pairwise matrix of scatter plots together with histograms for the individual features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris.feature_names)\n",
    "\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
    "                           marker='o', hist_kwds={'bins': 20}, s=60,\n",
    "                           alpha=.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5450e",
   "metadata": {},
   "source": [
    "### [Real-world datasets](https://scikit-learn.org/stable/datasets/real_world.html)\n",
    "\n",
    "In addition to the toy data, scikit-learn has loader functions for some commonly used larger sets. For example, the [Olivetti faces](https://scikit-learn.org/stable/datasets/real_world.html#the-olivetti-faces-dataset) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "olivetti = fetch_olivetti_faces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61acdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05edfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b051d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09be1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(olivetti.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e499d",
   "metadata": {},
   "source": [
    "### [Data generators](https://scikit-learn.org/stable/datasets/sample_generators.html)\n",
    "\n",
    "Sometimes we want to generate synthetic data to test clustering and regression methods. scikit-learn provides a number of helpful functions to do this for us.\n",
    "\n",
    "For example, we can create a classification dataset consisting of a number of blobs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# 100 data points, 2 features, 3 blobs\n",
    "blobs_X, blobs_y = make_blobs(100, 2, centers=3)\n",
    "\n",
    "# split off a test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(blobs_X, blobs_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0],X_train[:,1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb3586",
   "metadata": {},
   "source": [
    "### [external datasets](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#loading-from-external-datasets)\n",
    "\n",
    "There are of course many other sources of data that we might want to use.\n",
    "\n",
    "scikit-learn provides a direct interface to the [OpenML](https://www.openml.org/home) repository, so it is very easy to make use of these datasets in your work. See [here](https://scikit-learn.org/stable/datasets/loading_other_datasets.html#downloading-datasets-from-the-openml-org-repository) for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mice = fetch_openml(name='miceprotein', version=4)\n",
    "\n",
    "mice.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274d47c",
   "metadata": {},
   "source": [
    "To load data from a CSV file, we would use the pandas functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "codon = pd.read_csv(\"codon_usage.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a DataFrame for the features\n",
    "codon_X = codon.iloc[:,5:]\n",
    "\n",
    "# Extract a Series for the target\n",
    "codon_y = codon.iloc[:,0]\n",
    "\n",
    "# We can now work in scikit-learn with these pandas objects \n",
    "# in exactly the same way as for the numpy arrays\n",
    "X_train, X_test, y_train, y_test = train_test_split(codon_X, codon_y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20794abc",
   "metadata": {},
   "source": [
    "### Other data resources\n",
    "\n",
    "Some other repositories you may find useful:\n",
    "\n",
    "https://paperswithcode.com/datasets\n",
    "\n",
    "https://archive.ics.uci.edu/ml/index.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9a008",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "There may be several preprocessing steps that we need to complete before data is ready to use in our chosen method. Below are a few processes that are commonly applied.\n",
    "\n",
    "Importantly, note that the any manipulations to the data (imputation, standardisation etc.) should be calculated on the *training* data only, although the same transformation needs to be applied to both the training and testing data. If we preprocess before the train/test split then we are contaminating the testing data with information from the training data, which is what we want to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31337a15",
   "metadata": {},
   "source": [
    "#### [Encoding categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)\n",
    "\n",
    "Categorical datatypes need to be transformed to numbers before scikit-learn can use them as features.\n",
    "\n",
    "In some datasets, this has already been done by assigning an integer value to each label. This is called *ordinal encoding*.\n",
    "\n",
    "Although this is fine for binary attributes and (possibly) features with a natural ordering, it may produce poor performance when the datatype is nominal, i.e. with no meaningful ordering for the labels.\n",
    "\n",
    "A better solution is to use *one-hot encoding* to replace a single integer feature with multiple binary features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f7c2c",
   "metadata": {},
   "source": [
    "#### [Imputation of missing values](https://scikit-learn.org/stable/modules/impute.html)\n",
    "\n",
    "Real-world datasets often contain missing values, represented in data as `\"?\"` or `nan`.\n",
    "\n",
    "We might choose to ignore (drop) rows that contain missing values. However, this wastes the rest of the information in that row.\n",
    "\n",
    "It may be more desirable to insert a guess in place of the missing values. scikit-learn provides some simple and more complex methods to do this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a46bda",
   "metadata": {},
   "source": [
    "#### [Standardisation](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)\n",
    "\n",
    "Once we have encoded categorical features and imputed missing values, it may be necessary to center the data and transform it so that all features are equivalent in some way (for example, have equal variance). This is a requirement for some machine learning methods to operate correctly.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322268d",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Download the `autoMpg` dataset from OpenML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mpg = fetch_openml(name='autoMpg',version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c37773",
   "metadata": {},
   "source": [
    "See https://www.openml.org/d/196 for a description of the data.\n",
    "\n",
    "The `origin` feature is a nominal value coded as an integer.\n",
    "Use one-hot encoding to turn this column into multiple binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0c5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23e968f0",
   "metadata": {},
   "source": [
    "The `horsepower` feature has 6 missing values. Can you impute them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643da0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c1cb95",
   "metadata": {},
   "source": [
    "Finally, standardise the dataset using a method of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470613b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
